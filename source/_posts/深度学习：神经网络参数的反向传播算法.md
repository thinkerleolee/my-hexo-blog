---
title: 深度学习：神经网络参数的反向传播算法
date: 2019-01-28 10:20:07
tags:
    - 机器学习
    - 算法
    - 深度学习
    - 神经网络
    - 反向传播
categories:
    - 深度学习
mathjax: true
---

- **神经网络模型的代价函数是什么？**

    假设神经网络的训练样本有$m$个

    每个包含一组输入$x$和一组输出信号$y$

    $L$表示神经网络层数

    $S_I$表示每层的neuron个数($S_l$表示输出层神经元个数)

    $S_L$代表最后一层中处理单元的个数。

    将神经网络的分类定义为两种情况：二类分类和多类分类，

    二类分类：$S_L=0, y=0, or, 1$表示哪一类；

    $K$类分类：$S_L=k, y_i = 1$表示分到第$i$类；$(k>2)$

    在逻辑回归中，代价函数为：

    ![逻辑回归-代价函数.png](https://i.loli.net/2019/01/28/5c4e7148aeaab.png)

    在逻辑回归中，我们只有一个输出变量，又称为标量，也只有一个因变量$y$。

    在神经网络中，有多个输出变量，我们的$h_\theta(x)$是一个维度为$K$的向量。训练集中的因变量也是同样维度的一个量，因此代价函数会更为复杂：

    ![神经网络-代价函数.png](https://i.loli.net/2019/01/28/5c4e7148e26b9.png)

    对于每一行特征，我们都会给出$K$个预测，我们可以利用循环，对于每一行特征都预测$K$个不同结果，然后利用循环在$K$个预测中选择可能性最高的那个，然后与$y$中的实际数据进行比较。

    正则化的那一项只是排除了每一层$\theta_0​$后，每一层的$\theta​$ 矩阵的和。最里层的循环$j​$循环所有的行（由$s_l​$ +1 层的激活单元数决定），循环$i​$则循环所有的列，由该层（$s_l​$层）的激活单元数所决定。即：$h_\theta(x)​$与真实值之间的距离为每个样本-每个类输出的加和，对参数进行regularization的bias项处理所有参数的平方和。

    ![神经网络-代价函数推导.png](https://i.loli.net/2019/01/28/5c4e71496f005.png)

- **什么是反向传播算法？**

    反向传播（英语：Backpropagation，缩写为BP）是“误差反向传播”的简称，**是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法**。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。

    之前我们在计算神经网络预测结果的时候采用了一种正向传播方法：
    从第一层开始一层一层计算，直到最后一层$h_\theta(x)$

    现在，我们为了计算代价函数中的偏导数$\frac{\partial}{\partial\Theta^{(l)}_{ij} }J\left(\Theta\right)$，我们需要采用这种反向传播算法。

    首先，我们要计算最后一层的误差，然后一层层地反向求出各层的误差，直到隐藏层的第一层。

    举个栗子：

    假设我们训练集数量$m=1$，样本为$(x^{1},y^{1})$我们的神经网络是一个四层的神经网络，其$K=4,S_L=4,L=4$

    图示:

    ![神经网络-向前传播.png](https://i.loli.net/2019/01/28/5c4e714970723.png)

    训练样本向前传播以后，我们从最后一层的误差开始计算，误差是激活单元的预测值$(a^{(4)})$与实际值$(y^{k})$之间的误差（$k=1:k$）。我们用$\delta$ 来表示误差。

    则： $\delta^{(4)}=a^4-y$,我们算出了最后一层的误差向量$\delta ^{(4)}$。

    然后我们利用最后一层的误差向量$\delta ^{(4)}$来计算前一层的误差$\delta^{(3)}=\left({\Theta^{(3)} }\right)^{T}\delta^{(4)}\ast g'\left(z^{(3)}\right)$其中 $g'(z^{(3)})$是 $Sigmod$ 函数的导数，$g'(z^{(3)})=a^{(3)}\ast(1-a^{(3)})$。而$(θ^{(3)})^{T}\delta^{(4)}$则是权重导致的误差的和。

    下一步是继续计算第二层的误差： $ \delta^{(2)}=(\Theta^{(2)})^{T}\delta^{(3)}\ast g'(z^{(2)})$ 因为第一层是输入变量，不存在误差。我们有了所有的误差的表达式后，便可以计算代价函数的偏导数了，假设$λ=0$，即我们不做任何正则化处理时有： $\frac{\partial}{\partial\Theta_{ij}^{(l)} }J(\Theta)=a_{j}^{(l)} \delta_{i}^{l+1}$

    重要的是清楚地知道上面式子中上下标的含义：

    $l$ 代表目前所计算的是第几层。

    $j$ 代表目前计算层中的激活单元的下标，也将是下一层的第$j$个输入变量的下标。

    $i$ 代表下一层中误差单元的下标，是受到权重矩阵中第$i$行影响的下一层中的误差单元的下标。

    如果我们考虑正则化处理，并且我们的训练集是一个特征矩阵而非向量。在上面的特殊情况中，我们需要计算每一层的误差单元来计算代价函数的偏导数。在更为一般的情况中，我们同样需要计算每一层的误差单元，但是我们需要为整个训练集计算误差单元，此时的误差单元也是一个矩阵，我们**用$\Delta^{(l)}_{ij}$来表示这个误差矩阵**。**第 $l$ 层的第 $i$ 个激活单元受到第 $j$ 个参数影响而导致的误差**。

    我们的算法表示为：

    ![神经网络-反向传播表示.png](https://i.loli.net/2019/01/28/5c4e71493b34f.png)

    即首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。

    在求出了$\Delta_{ij}^{(l)}$之后，我们便可以计算代价函数的偏导数了，计算方法如下： $D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}$ ${if}; j \neq 0$

    $D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}$ ${if}; j = 0$

    在Octave 中，如果我们要使用 fminuc这样的优化算法来求解求出权重矩阵，我们需要将矩阵首先展开成为向量，在利用算法求出最优解后再重新转换回矩阵。

    假设我们有三个权重矩阵，Theta1，Theta2 和 Theta3，尺寸分别为 10*11，10*11 和1*11， 下面的代码可以实现这样的转换：
    
    ```matlab
    thetaVec = [Theta1(:) ; Theta2(:) ; Theta3(:)]

    ...optimization using functions like fminuc...

    Theta1 = reshape(thetaVec(1:110, 10, 11);

    Theta2 = reshape(thetaVec(111:220, 10, 11);

    Theta1 = reshape(thetaVec(221:231, 1, 11);
    ```
    
- **如何初始化所有参数呢？**

    任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非0的数，结果也是一样的。

    我们通常初始参数为正负ε之间的随机值，假设我们要随机初始一个尺寸为10×11的参数矩阵，代码如下：

    ```
        Theta1 = rand(10, 11) * (2*eps) – eps
    ```

- **什么是使用神经网络的综合步骤？**

    总结一下：

    网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。

    第一层的单元数即我们训练集的特征数量。

    最后一层的单元数是我们训练集的结果的类的数量。

    如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。

    我们真正要决定的是隐藏层的层数和每个中间层的单元数。

    - 训练神经网络：

        1、参数的随机初始化

        2、利用正向传播方法计算所有的$h_{\theta}(x)$

        3、编写计算代价函数 $J$ 的代码

        4、利用反向传播方法计算所有偏导数

        ![神经网络-循环训练神经网络.png](https://i.loli.net/2019/01/28/5c4e71493676c.png)

        5、利用数值检验方法检验这些偏导数

        6、使用优化算法来最小化代价函数

