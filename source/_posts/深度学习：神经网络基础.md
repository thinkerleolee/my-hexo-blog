---
title: 深度学习：神经网络基础
date: 2019-01-28 10:10:07
tags:
    - 机器学习
    - 算法
    - 深度学习
    - 神经网络
categories:
    - 深度学习
mathjax: true
---

## 非线性假设

- **线性回归和逻辑回归模型的缺点是什么,为什么需要神经网络?**

    我们之前学的，无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。

    下面是一个例子：

    ![神经网络-线性模型缺点.png](https://i.loli.net/2019/01/28/5c4e72b3b30da.png)

    当我们使用x1, x2 的多次项式进行预测时，我们可以应用的很好。 之前我们已经看到过，使用非线性的多项式项，能够帮助我们建立更好的分类模型。假设我们有非常多的特征，例如大于100个变量，我们希望用这100个特征来构建一个非线性的多项式模型，结果将是数量非常惊人的特征组合，即便我们只采用两两特征的组合$(x_1x_2+x_1x_3+x_1x_4+...+x_2x_3+x_2x_4+...+x_{99}x_{100})$，我们也会有接近5000个组合而成的特征。这对于一般的逻辑回归来说需要计算的特征太多了。

    假使我们采用的都是50x50像素的小图片，并且我们将所有的像素视为特征，则会有 2500个特征，如果我们要进一步将两两特征组合构成一个多项式模型，则会有约$2500^2/2$个（接近3百万个）特征。普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们需要更适合的机器学习算法---**神经网络**。

- **神经网络的基本结构有哪些？**

    ![神经网络-人工神经网络类型.png](https://i.loli.net/2019/01/28/5c4e72b398b21.png)

    **注：双隐层及以上的区域决策类型均能表达任意形状**
    
- **神经网络的模型怎么表示？**

    为了构建神经网络模型，我们需要首先思考大脑中的神经网络是怎样的？每一个神经元都可以被认为是一个处理单元/神经核（processing unit/Nucleus），它含有许多**输入/树突（input/Dendrite）**，并且有一个**输出/轴突（output/Axon）**。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络。

    如图：

    ![神经网络-生物神经模型.png](https://i.loli.net/2019/01/28/5c4e72b3b9933.png)

    其中：

    **树突** 对应 **数据输入**

    **轴突** 对应 **数据输出**

    神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元，activation unit）采纳一些特征作为输出，并且根据本身的模型提供一个输出。下图是一个以逻辑回归模型作为自身学习模型的神经元示例，在神经网络中，参数又可被成为**权重（weight）**。

    ![神经网络-权重表示.png](https://i.loli.net/2019/01/28/5c4e72b3b52b4.png)

    人们在人类神经结构的基础上，发明了一种类似神经元仿生的模型：人工神经网络

    如图：

    ![神经网络-人工神经网络模型.png](https://i.loli.net/2019/01/28/5c4e72b3b7680.png)

    其中$x_1, x_2, x_3$是输入单元（input units），我们将**原始数据**输入给它们。 $a_1, a_2, a_3$是中间单元，它们负责将数据**进行处理**，然后呈**递到下一层**。 最后是输出单元，它**负责计算**$h_θ(x)$。

    下面引入一些标记法来帮助描述模型： $a_{i}^{\left( j \right)}$ 代表第$j$ 层的第 $i$ 个激活单元。${ {\theta }^{\left( j \right)} }$代表从第 $j$ 层映射到第$j+1$ 层时的权重的矩阵，例如${ {\theta }^{\left( 1 \right)} }$代表从第一层映射到第二层的权重的矩阵。其尺寸为：以第 $j+1$层的激活单元数量为行数，以第 $j$ 层的激活单元数加一为列数的矩阵。例如：上图所示的神经网络中${ {\theta }^{\left( 1 \right)} }$的尺寸为 3*4。

    对于上图所示的模型，激活单元和输出分别表达为：

    $$
    a_{1}^{(2)}=g(\Theta _{10}^{(1)}{ {x}{0} }+\Theta _{11}^{(1)}{ {x}{1} }+\Theta _{12}^{(1)}{ {x}{2} }+\Theta _{13}^{(1)}{ {x}{3} })
    $$
    
    $$
    a_{2}^{(2)}=g(\Theta _{20}^{(1)}{ {x}{0} }+\Theta _{21}^{(1)}{ {x}{1} }+\Theta _{22}^{(1)}{ {x}{2} }+\Theta _{23}^{(1)}{ {x}{3} })
    $$
    
    $$
    a_{3}^{(2)}=g(\Theta _{30}^{(1)}{ {x}{0} }+\Theta _{31}^{(1)}{ {x}{1} }+\Theta _{32}^{(1)}{ {x}{2} }+\Theta _{33}^{(1)}{ {x}{3} })$$
    
    $$
    { {h}_{\Theta } }(x)=g(\Theta _{10}^{(2)}a{0}^{(2)}+\Theta _{11}^{(2)}a{1}^{(2)}+\Theta _{12}^{(2)}a{2}^{(2)}+\Theta _{13}^{(2)}a{3}^{(2)})
    $$

    相对于使用循环来编码，利用向量化的方法会使得计算更为简便。以上面的神经网络为例，试着计算第二层的值：

    ![神经网络-向量化计算.png](https://i.loli.net/2019/01/28/5c4e72b3a143c.png)

    我们令 ${ {z}^{\left( 2 \right)} }={ {\theta }^{\left( 1 \right)} }x$，则 ${ {a}^{\left( 2 \right)} }=g({ {z}^{\left( 2 \right)} })$ ，计算后添加 $a_{0}^{\left( 2 \right)}=1$。 计算输出的值为：

    ![神经网络-向量计算.png](https://i.loli.net/2019/01/28/5c4e732dbe83c.png)

    我们令${ {z}^{\left( 3 \right)} }={ {\theta }^{\left( 2 \right)} }{ {a}^{\left( 2 \right)} }$，则 $h_\theta(x)={ {a}^{\left( 3 \right)} }=g({ {z}^{\left( 3 \right)} })$。 这只是针对训练集中一个训练实例所进行的计算。如果我们要对整个训练集进行计算，我们需要将训练集特征矩阵进行转置，使得同一个实例的特征都在同一列里。即： 
    ${ {z}^{\left( 2 \right)} }={ {\Theta }^{\left( 1 \right)} }\times { {X}^{T} }$

    ${ {a}^{\left( 2 \right)} }=g({ {z}^{\left( 2 \right)} })$