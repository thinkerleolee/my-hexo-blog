---
title: 机器学习算法：单变量线性回归
date: 2019-01-28 00:10:07
tags:
    - 机器学习
    - 算法
    - 线性回归
categories:
    - 机器学习
mathjax: true
---

## 单变量线性回归

- **什么是回归分析?**

    回归分析（Regression Analysis）是一种统计学上分析数据的方法，目的在于了解两个或多个变量间是否相关、相关方向与强度，并建立数学模型以便观察特定变量来预测研究者感兴趣的变量。更具体的来说，回归分析可以帮助人们了解在只有一个自变量变化时因变量的变化量。一般来说，通过回归分析我们可以由给出的自变量估计因变量的条件期望。

- **什么是线性回归？**

    线性回归（Linear regression）是利用称为线性回归方程的最小平方函數对一个或多个自变量和因变量之间关系进行建模的一种回归分析。 这种函数是一个或多个称为回归系数的模型参数的线性组合。 只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。

- **Andrew Ng课程中对于监督学习算法的表示是什么？**

    $m$代表训练集中实例的数量

    $x$代表特征/输入变量

    $y$代表目标变量/输出变量

    $(x,y)$代表训练集中的实例

    $(x^{(i)},y^{(i)})$代表第$i$个观察实例

    $h$代表学习算法的解决方案或函数成为假设。

    ![监督学习模型.png](https://i.loli.net/2019/01/28/5c4dde2865e7b.png)

- **单变量线性回归的数学表示是什么?**

    假设函数：

    $$
        h_\theta(x)=\theta_0+\theta_1x
    $$

    其中$\theta_0、\theta_1$是参数

    代价函数：

    $$
        J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
    $$

    我们的目的是使得代价函数$J(\theta_0,\theta_1)$取得最小值

    也就是求 $minimize~J(\theta_0, \theta_1)$


- **那么。。怎么求$minimize~J(\theta_0, \theta_1)$呢？**

    $minimize~J(\theta_0, \theta_1)$的图像，大概是这个样子的：

    ![代价函数图像.png](https://i.loli.net/2019/01/28/5c4dde80ca5be.png)

    这时候，我们就要请出**梯度下降**法了。梯度下降在之前数学基础中有相关的内容。

    ![梯度下降-山.jpg](https://i.loli.net/2019/01/28/5c4dde809da57.jpg)

    想象一下你正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转360度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。

    **简而言之，就是小碎步下山，每一步都选出下山最快的那个方向迈步子。**

    Andrew Ng对梯度下降的算法公式定义是：

    repeat until convergence
    {

    $$
    \theta_j := \theta_j-\alpha\frac{\alpha}{\alpha\theta_j}J(\theta_0,\theta_1)~~~~(for~j=0~and~j=1)
    $$
    
    }

    **其中a是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。**

    注意在小步迭代中，参数要**同步更新**

    ![梯度下降-同步更新.png](https://i.loli.net/2019/01/28/5c4dde80c8947.png)

    对于求 $minimize~J(\theta_0, \theta_1)$，用梯度下降法就是：

    repeat until convergence{

    $\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})$

    $\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x^{i}$

    }

    ![代价函数-梯度下降求值.png](https://i.loli.net/2019/01/28/5c4dde80e8441.png)

    其中蓝框和红框中的就是对参数求偏微分的结果。

- **梯度下降为什么会陷入局部最优解的问题？**

    ![梯度下降-山.jpg](https://i.loli.net/2019/01/28/5c4dde809da57.jpg)

    如图，在有多个局部最优解的时候，有时会得出不同的最优解结论